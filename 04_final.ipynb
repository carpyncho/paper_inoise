{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import itertools as it\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# sns.set()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "import joblib\n",
    "\n",
    "from libs.container import Container\n",
    "from libs.display import d\n",
    "from libs.experiment import KFoldExperiment, WithAnotherExperiment, roc, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu = joblib.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_pickle(\"data/scaled/sample.pkl.bz2\")\n",
    "sample[\"tile\"] = sample[\"id\"].apply(lambda i: \"b\" + str(i)[1:4])\n",
    "\n",
    "no_features = [\"id\", \"vs_catalog\", \"vs_type\", \"ra_k\", \"dec_k\", \"tile\", \"cls\"] \n",
    "X_columns = [c for c in sample.columns if c not in no_features]\n",
    "\n",
    "grouped = sample.groupby(\"tile\")\n",
    "data = Container({k: grouped.get_group(k).copy() for k in grouped.groups.keys()})\n",
    "\n",
    "del grouped, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_PARAMS = {\n",
    "    'max_features': None, 'min_samples_split': 10, 'n_jobs': cpu, \n",
    "    'criterion': 'entropy', 'n_estimators': 500}\n",
    "\n",
    "sX_columns = [\n",
    "    'Beyond1Std',\n",
    "    'Eta_e',\n",
    "    'Freq1_harmonics_amplitude_0',\n",
    "    'LinearTrend',\n",
    "    'MaxSlope',\n",
    "    'Mean',\n",
    "    'Meanvariance',\n",
    "    'Psi_eta',\n",
    "    'Rcs',\n",
    "    'c89_m2',\n",
    "    'cnt',\n",
    "    'n09_c3',\n",
    "    'n09_hk_color',\n",
    "    'n09_m2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = {}\n",
    "for ta, tb in it.combinations(data.keys(), 2):\n",
    "    k = \"{}_{}\".format(ta, tb)\n",
    "    df = pd.concat([data[ta], data[tb]])\n",
    "    \n",
    "    cls = {name: idx for idx, name in enumerate(df.tile.unique())}\n",
    "    df[\"cls\"] = df.tile.apply(cls.get)\n",
    "    \n",
    "    combs[k] = df\n",
    "\n",
    "data = Container(combs)\n",
    "del combs\n",
    "\n",
    "cls = {0: 0, 1: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b396 vs b264\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.90      0.89       999\n",
      "         1.0       0.89      0.87      0.88      1000\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      1999\n",
      "   macro avg       0.89      0.89      0.89      1999\n",
      "weighted avg       0.89      0.89      0.89      1999\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b261 vs b248\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.88      0.87       998\n",
      "         1.0       0.88      0.84      0.86       998\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1996\n",
      "   macro avg       0.87      0.86      0.86      1996\n",
      "weighted avg       0.87      0.86      0.86      1996\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b248 vs b220\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.90      0.90       998\n",
      "         1.0       0.90      0.89      0.90      1000\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1998\n",
      "   macro avg       0.90      0.90      0.90      1998\n",
      "weighted avg       0.90      0.90      0.90      1998\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b396 vs b261\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.84      0.86       999\n",
      "         1.0       0.84      0.89      0.87       998\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1997\n",
      "   macro avg       0.86      0.86      0.86      1997\n",
      "weighted avg       0.86      0.86      0.86      1997\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b396 vs b262\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.82      0.86       999\n",
      "         1.0       0.84      0.90      0.86       996\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1995\n",
      "   macro avg       0.86      0.86      0.86      1995\n",
      "weighted avg       0.86      0.86      0.86      1995\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b396 vs b263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.93      0.91       999\n",
      "         1.0       0.92      0.90      0.91      1000\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      1999\n",
      "   macro avg       0.91      0.91      0.91      1999\n",
      "weighted avg       0.91      0.91      0.91      1999\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b396 vs b220\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.91      0.90       999\n",
      "         1.0       0.91      0.89      0.90      1000\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1999\n",
      "   macro avg       0.90      0.90      0.90      1999\n",
      "weighted avg       0.90      0.90      0.90      1999\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b396 vs b247\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.86      0.83       999\n",
      "         1.0       0.85      0.79      0.82      1000\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      1999\n",
      "   macro avg       0.83      0.83      0.83      1999\n",
      "weighted avg       0.83      0.83      0.83      1999\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b248 vs b262\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.84      0.86       998\n",
      "         1.0       0.85      0.88      0.86       996\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1994\n",
      "   macro avg       0.86      0.86      0.86      1994\n",
      "weighted avg       0.86      0.86      0.86      1994\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b248 vs b263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.92      0.91       998\n",
      "         1.0       0.92      0.89      0.90      1000\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      1998\n",
      "   macro avg       0.91      0.91      0.91      1998\n",
      "weighted avg       0.91      0.91      0.91      1998\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b248 vs b264\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.89      0.88       998\n",
      "         1.0       0.89      0.87      0.88      1000\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      1998\n",
      "   macro avg       0.88      0.88      0.88      1998\n",
      "weighted avg       0.88      0.88      0.88      1998\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b264 vs b277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.91      0.93      1000\n",
      "         1.0       0.91      0.94      0.93       996\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1996\n",
      "   macro avg       0.93      0.93      0.93      1996\n",
      "weighted avg       0.93      0.93      0.93      1996\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b262 vs b278\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.84      0.86       996\n",
      "         1.0       0.85      0.90      0.87       994\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      1990\n",
      "   macro avg       0.87      0.87      0.87      1990\n",
      "weighted avg       0.87      0.87      0.87      1990\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b248 vs b247\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.72      0.71       998\n",
      "         1.0       0.71      0.70      0.71      1000\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      1998\n",
      "   macro avg       0.71      0.71      0.71      1998\n",
      "weighted avg       0.71      0.71      0.71      1998\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b261 vs b278\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.86      0.87       998\n",
      "         1.0       0.86      0.90      0.88       994\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      1992\n",
      "   macro avg       0.88      0.88      0.88      1992\n",
      "weighted avg       0.88      0.88      0.88      1992\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b263 vs b277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.92      0.93      1000\n",
      "         1.0       0.92      0.95      0.94       996\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1996\n",
      "   macro avg       0.94      0.93      0.93      1996\n",
      "weighted avg       0.94      0.93      0.93      1996\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b262 vs b277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.84      0.87       996\n",
      "         1.0       0.85      0.91      0.88       996\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      1992\n",
      "   macro avg       0.88      0.88      0.88      1992\n",
      "weighted avg       0.88      0.88      0.88      1992\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b264 vs b234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.87      0.88      1000\n",
      "         1.0       0.88      0.90      0.89      1000\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      2000\n",
      "   macro avg       0.89      0.89      0.89      2000\n",
      "weighted avg       0.89      0.89      0.89      2000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b277 vs b278\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.74      0.74       996\n",
      "         1.0       0.74      0.74      0.74       994\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      1990\n",
      "   macro avg       0.74      0.74      0.74      1990\n",
      "weighted avg       0.74      0.74      0.74      1990\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b261 vs b247\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.89      0.86       998\n",
      "         1.0       0.88      0.83      0.86      1000\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1998\n",
      "   macro avg       0.86      0.86      0.86      1998\n",
      "weighted avg       0.86      0.86      0.86      1998\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b396 vs b248\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.84      0.82       999\n",
      "         1.0       0.84      0.80      0.82       998\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      1997\n",
      "   macro avg       0.82      0.82      0.82      1997\n",
      "weighted avg       0.82      0.82      0.82      1997\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b261 vs b220\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.89      0.88       998\n",
      "         1.0       0.88      0.87      0.88      1000\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      1998\n",
      "   macro avg       0.88      0.88      0.88      1998\n",
      "weighted avg       0.88      0.88      0.88      1998\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b234 vs b278\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.91      0.92      1000\n",
      "         1.0       0.91      0.94      0.92       994\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1994\n",
      "   macro avg       0.92      0.92      0.92      1994\n",
      "weighted avg       0.92      0.92      0.92      1994\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b262 vs b234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.91      0.89       996\n",
      "         1.0       0.91      0.87      0.89      1000\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      1996\n",
      "   macro avg       0.89      0.89      0.89      1996\n",
      "weighted avg       0.89      0.89      0.89      1996\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b261 vs b263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.92      0.90       998\n",
      "         1.0       0.91      0.89      0.90      1000\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1998\n",
      "   macro avg       0.90      0.90      0.90      1998\n",
      "weighted avg       0.90      0.90      0.90      1998\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b261 vs b262\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.62      0.62       998\n",
      "         1.0       0.62      0.63      0.63       996\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      1994\n",
      "   macro avg       0.63      0.63      0.63      1994\n",
      "weighted avg       0.63      0.63      0.63      1994\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b247 vs b220\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.92      0.92      1000\n",
      "         1.0       0.92      0.91      0.92      1000\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      2000\n",
      "   macro avg       0.92      0.92      0.92      2000\n",
      "weighted avg       0.92      0.92      0.92      2000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b263 vs b278\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.93      0.93      1000\n",
      "         1.0       0.92      0.93      0.93       994\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1994\n",
      "   macro avg       0.93      0.93      0.93      1994\n",
      "weighted avg       0.93      0.93      0.93      1994\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b247 vs b264\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.90      0.89      1000\n",
      "         1.0       0.90      0.89      0.89      1000\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      2000\n",
      "   macro avg       0.89      0.89      0.89      2000\n",
      "weighted avg       0.89      0.89      0.89      2000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b248 vs b278\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.90      0.91       998\n",
      "         1.0       0.90      0.92      0.91       994\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      1992\n",
      "   macro avg       0.91      0.91      0.91      1992\n",
      "weighted avg       0.91      0.91      0.91      1992\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b396 vs b278\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.88      0.90       999\n",
      "         1.0       0.88      0.93      0.91       994\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1993\n",
      "   macro avg       0.91      0.90      0.90      1993\n",
      "weighted avg       0.91      0.90      0.90      1993\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b396 vs b277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.89      0.91       999\n",
      "         1.0       0.89      0.93      0.91       996\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      1995\n",
      "   macro avg       0.91      0.91      0.91      1995\n",
      "weighted avg       0.91      0.91      0.91      1995\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b261 vs b234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.91      0.89       998\n",
      "         1.0       0.91      0.86      0.89      1000\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      1998\n",
      "   macro avg       0.89      0.89      0.89      1998\n",
      "weighted avg       0.89      0.89      0.89      1998\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b248 vs b234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.71      0.72       998\n",
      "         1.0       0.72      0.74      0.73      1000\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      1998\n",
      "   macro avg       0.73      0.73      0.73      1998\n",
      "weighted avg       0.73      0.73      0.73      1998\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b263 vs b264\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.57      0.57      1000\n",
      "         1.0       0.57      0.56      0.56      1000\n",
      "\n",
      "   micro avg       0.57      0.57      0.57      2000\n",
      "   macro avg       0.57      0.57      0.57      2000\n",
      "weighted avg       0.57      0.57      0.57      2000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b263 vs b247\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.90      0.90      1000\n",
      "         1.0       0.90      0.91      0.91      1000\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      2000\n",
      "   macro avg       0.90      0.90      0.90      2000\n",
      "weighted avg       0.90      0.90      0.90      2000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b248 vs b277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.89      0.90       998\n",
      "         1.0       0.89      0.92      0.90       996\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1994\n",
      "   macro avg       0.90      0.90      0.90      1994\n",
      "weighted avg       0.90      0.90      0.90      1994\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b396 vs b234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.85      0.84       999\n",
      "         1.0       0.84      0.82      0.83      1000\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      1999\n",
      "   macro avg       0.83      0.83      0.83      1999\n",
      "weighted avg       0.83      0.83      0.83      1999\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b262 vs b264\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.91      0.90       996\n",
      "         1.0       0.91      0.87      0.89      1000\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      1996\n",
      "   macro avg       0.89      0.89      0.89      1996\n",
      "weighted avg       0.89      0.89      0.89      1996\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b264 vs b220\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.91      0.91      1000\n",
      "         1.0       0.91      0.91      0.91      1000\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      2000\n",
      "   macro avg       0.91      0.91      0.91      2000\n",
      "weighted avg       0.91      0.91      0.91      2000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b262 vs b263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.93      0.91       996\n",
      "         1.0       0.92      0.89      0.91      1000\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      1996\n",
      "   macro avg       0.91      0.91      0.91      1996\n",
      "weighted avg       0.91      0.91      0.91      1996\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b262 vs b220\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.87      0.87       996\n",
      "         1.0       0.87      0.88      0.87      1000\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      1996\n",
      "   macro avg       0.87      0.87      0.87      1996\n",
      "weighted avg       0.87      0.87      0.87      1996\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b262 vs b247\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.87      0.85       996\n",
      "         1.0       0.87      0.83      0.85      1000\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      1996\n",
      "   macro avg       0.85      0.85      0.85      1996\n",
      "weighted avg       0.85      0.85      0.85      1996\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b263 vs b220\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.91      0.92      1000\n",
      "         1.0       0.91      0.92      0.92      1000\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      2000\n",
      "   macro avg       0.92      0.92      0.92      2000\n",
      "weighted avg       0.92      0.92      0.92      2000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b247 vs b278\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.89      0.90      1000\n",
      "         1.0       0.89      0.91      0.90       994\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1994\n",
      "   macro avg       0.90      0.90      0.90      1994\n",
      "weighted avg       0.90      0.90      0.90      1994\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b277 vs b220\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.95      0.94       996\n",
      "         1.0       0.95      0.92      0.93      1000\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1996\n",
      "   macro avg       0.94      0.93      0.93      1996\n",
      "weighted avg       0.94      0.93      0.93      1996\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b234 vs b220\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.91      0.91      1000\n",
      "         1.0       0.91      0.90      0.90      1000\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      2000\n",
      "   macro avg       0.90      0.90      0.90      2000\n",
      "weighted avg       0.90      0.90      0.90      2000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b247 vs b277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.88      0.90      1000\n",
      "         1.0       0.88      0.92      0.90       996\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1996\n",
      "   macro avg       0.90      0.90      0.90      1996\n",
      "weighted avg       0.90      0.90      0.90      1996\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b278 vs b220\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.93      0.93       994\n",
      "         1.0       0.93      0.92      0.93      1000\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1994\n",
      "   macro avg       0.93      0.93      0.93      1994\n",
      "weighted avg       0.93      0.93      0.93      1994\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b261 vs b277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.85      0.87       998\n",
      "         1.0       0.86      0.91      0.88       996\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      1994\n",
      "   macro avg       0.88      0.88      0.88      1994\n",
      "weighted avg       0.88      0.88      0.88      1994\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b264 vs b278\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.92      0.92      1000\n",
      "         1.0       0.92      0.92      0.92       994\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1994\n",
      "   macro avg       0.92      0.92      0.92      1994\n",
      "weighted avg       0.92      0.92      0.92      1994\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b234 vs b277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.90      0.92      1000\n",
      "         1.0       0.91      0.94      0.92       996\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1996\n",
      "   macro avg       0.92      0.92      0.92      1996\n",
      "weighted avg       0.92      0.92      0.92      1996\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b247 vs b234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.71      0.72      1000\n",
      "         1.0       0.72      0.73      0.72      1000\n",
      "\n",
      "   micro avg       0.72      0.72      0.72      2000\n",
      "   macro avg       0.72      0.72      0.72      2000\n",
      "weighted avg       0.72      0.72      0.72      2000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b261 vs b264\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.90      0.89       998\n",
      "         1.0       0.90      0.88      0.89      1000\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      1998\n",
      "   macro avg       0.89      0.89      0.89      1998\n",
      "weighted avg       0.89      0.89      0.89      1998\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "b263 vs b234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.89      0.91      1000\n",
      "         1.0       0.89      0.92      0.91      1000\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      2000\n",
      "   macro avg       0.91      0.91      0.91      2000\n",
      "weighted avg       0.91      0.91      0.91      2000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "CPU times: user 4h 34min 16s, sys: 8min 54s, total: 4h 43min 10s\n",
      "Wall time: 33min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = {}\n",
    "for c in data.keys():\n",
    "    print(\"{} vs {}\".format(*c.split(\"_\")))\n",
    "    rf = KFoldExperiment(\n",
    "        clf=RandomForestClassifier(**RF_PARAMS), clsnum=cls, \n",
    "        data=data, pcls=1, ncls=0, X_columns=sX_columns, y_column=\"cls\")\n",
    "    rf = rf(c, nfolds=10)\n",
    "    results[c] = rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b220',\n",
       " 'b234',\n",
       " 'b247',\n",
       " 'b248',\n",
       " 'b261',\n",
       " 'b262',\n",
       " 'b263',\n",
       " 'b264',\n",
       " 'b277',\n",
       " 'b278',\n",
       " 'b396'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tiles = set(it.chain(*[k.split(\"_\") for k in data.keys()]))\n",
    "all_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b220',\n",
       " 'b234',\n",
       " 'b247',\n",
       " 'b248',\n",
       " 'b261',\n",
       " 'b262',\n",
       " 'b263',\n",
       " 'b264',\n",
       " 'b277',\n",
       " 'b278',\n",
       " 'b396']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(all_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for k, r in sorted(results.items()):\n",
    "    ta, tb = k.split(\"_\")\n",
    "    row = {\n",
    "        \"Tile A\": ta, \n",
    "        \"Tile B\": tb,\n",
    "        \"Prec.\": metrics.precision_score(r.y_test, r.predictions),\n",
    "        \"Recall\": metrics.recall_score(r.y_test, r.predictions),\n",
    "        \"AUC\": r.roc_auc}\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tile A</th>\n",
       "      <th>Tile B</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>Recall</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b234</td>\n",
       "      <td>b220</td>\n",
       "      <td>0.909828</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>0.970149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b234</td>\n",
       "      <td>b277</td>\n",
       "      <td>0.906280</td>\n",
       "      <td>0.941767</td>\n",
       "      <td>0.976709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b234</td>\n",
       "      <td>b278</td>\n",
       "      <td>0.909268</td>\n",
       "      <td>0.937626</td>\n",
       "      <td>0.978964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b247</td>\n",
       "      <td>b220</td>\n",
       "      <td>0.918511</td>\n",
       "      <td>0.913000</td>\n",
       "      <td>0.974521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b247</td>\n",
       "      <td>b234</td>\n",
       "      <td>0.717520</td>\n",
       "      <td>0.729000</td>\n",
       "      <td>0.822018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b247</td>\n",
       "      <td>b264</td>\n",
       "      <td>0.898785</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.964052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b247</td>\n",
       "      <td>b277</td>\n",
       "      <td>0.880383</td>\n",
       "      <td>0.923695</td>\n",
       "      <td>0.968849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b247</td>\n",
       "      <td>b278</td>\n",
       "      <td>0.892822</td>\n",
       "      <td>0.913481</td>\n",
       "      <td>0.971790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b248</td>\n",
       "      <td>b220</td>\n",
       "      <td>0.898288</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.963729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b248</td>\n",
       "      <td>b234</td>\n",
       "      <td>0.722763</td>\n",
       "      <td>0.743000</td>\n",
       "      <td>0.813862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b248</td>\n",
       "      <td>b247</td>\n",
       "      <td>0.713706</td>\n",
       "      <td>0.703000</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>b248</td>\n",
       "      <td>b262</td>\n",
       "      <td>0.845930</td>\n",
       "      <td>0.876506</td>\n",
       "      <td>0.944090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>b248</td>\n",
       "      <td>b263</td>\n",
       "      <td>0.921080</td>\n",
       "      <td>0.887000</td>\n",
       "      <td>0.970596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>b248</td>\n",
       "      <td>b264</td>\n",
       "      <td>0.886965</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.958738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b248</td>\n",
       "      <td>b277</td>\n",
       "      <td>0.890185</td>\n",
       "      <td>0.919679</td>\n",
       "      <td>0.970049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>b248</td>\n",
       "      <td>b278</td>\n",
       "      <td>0.901381</td>\n",
       "      <td>0.919517</td>\n",
       "      <td>0.974071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>b261</td>\n",
       "      <td>b220</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.874000</td>\n",
       "      <td>0.947592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>b261</td>\n",
       "      <td>b234</td>\n",
       "      <td>0.910432</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.963223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>b261</td>\n",
       "      <td>b247</td>\n",
       "      <td>0.880550</td>\n",
       "      <td>0.833000</td>\n",
       "      <td>0.945345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>b261</td>\n",
       "      <td>b248</td>\n",
       "      <td>0.879958</td>\n",
       "      <td>0.844689</td>\n",
       "      <td>0.946786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>b261</td>\n",
       "      <td>b262</td>\n",
       "      <td>0.624135</td>\n",
       "      <td>0.633534</td>\n",
       "      <td>0.661776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>b261</td>\n",
       "      <td>b263</td>\n",
       "      <td>0.914521</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.969078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>b261</td>\n",
       "      <td>b264</td>\n",
       "      <td>0.899384</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0.959428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>b261</td>\n",
       "      <td>b277</td>\n",
       "      <td>0.857550</td>\n",
       "      <td>0.906627</td>\n",
       "      <td>0.950203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>b261</td>\n",
       "      <td>b278</td>\n",
       "      <td>0.861702</td>\n",
       "      <td>0.896378</td>\n",
       "      <td>0.947162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>b262</td>\n",
       "      <td>b220</td>\n",
       "      <td>0.870647</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.945894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>b262</td>\n",
       "      <td>b234</td>\n",
       "      <td>0.909186</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.962012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>b262</td>\n",
       "      <td>b247</td>\n",
       "      <td>0.865765</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.941996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>b262</td>\n",
       "      <td>b263</td>\n",
       "      <td>0.924274</td>\n",
       "      <td>0.891000</td>\n",
       "      <td>0.971047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>b262</td>\n",
       "      <td>b264</td>\n",
       "      <td>0.910417</td>\n",
       "      <td>0.874000</td>\n",
       "      <td>0.959193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>b262</td>\n",
       "      <td>b277</td>\n",
       "      <td>0.853497</td>\n",
       "      <td>0.906627</td>\n",
       "      <td>0.946610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>b262</td>\n",
       "      <td>b278</td>\n",
       "      <td>0.845574</td>\n",
       "      <td>0.903421</td>\n",
       "      <td>0.945155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>b263</td>\n",
       "      <td>b220</td>\n",
       "      <td>0.914683</td>\n",
       "      <td>0.922000</td>\n",
       "      <td>0.979728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>b263</td>\n",
       "      <td>b234</td>\n",
       "      <td>0.894380</td>\n",
       "      <td>0.923000</td>\n",
       "      <td>0.970767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>b263</td>\n",
       "      <td>b247</td>\n",
       "      <td>0.898522</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.970296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>b263</td>\n",
       "      <td>b264</td>\n",
       "      <td>0.568089</td>\n",
       "      <td>0.559000</td>\n",
       "      <td>0.591133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>b263</td>\n",
       "      <td>b277</td>\n",
       "      <td>0.924510</td>\n",
       "      <td>0.946787</td>\n",
       "      <td>0.982383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>b263</td>\n",
       "      <td>b278</td>\n",
       "      <td>0.924774</td>\n",
       "      <td>0.927565</td>\n",
       "      <td>0.981689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>b264</td>\n",
       "      <td>b220</td>\n",
       "      <td>0.910643</td>\n",
       "      <td>0.907000</td>\n",
       "      <td>0.974507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>b264</td>\n",
       "      <td>b234</td>\n",
       "      <td>0.876098</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>0.964274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>b264</td>\n",
       "      <td>b277</td>\n",
       "      <td>0.914314</td>\n",
       "      <td>0.942771</td>\n",
       "      <td>0.974634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>b264</td>\n",
       "      <td>b278</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.923541</td>\n",
       "      <td>0.974427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>b277</td>\n",
       "      <td>b220</td>\n",
       "      <td>0.947531</td>\n",
       "      <td>0.921000</td>\n",
       "      <td>0.983049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>b277</td>\n",
       "      <td>b278</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.740443</td>\n",
       "      <td>0.849118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>b278</td>\n",
       "      <td>b220</td>\n",
       "      <td>0.934010</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.983777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>b396</td>\n",
       "      <td>b220</td>\n",
       "      <td>0.909369</td>\n",
       "      <td>0.893000</td>\n",
       "      <td>0.971480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>b396</td>\n",
       "      <td>b234</td>\n",
       "      <td>0.843077</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.922075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>b396</td>\n",
       "      <td>b247</td>\n",
       "      <td>0.852814</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.915497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>b396</td>\n",
       "      <td>b248</td>\n",
       "      <td>0.836134</td>\n",
       "      <td>0.797595</td>\n",
       "      <td>0.905387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>b396</td>\n",
       "      <td>b261</td>\n",
       "      <td>0.844762</td>\n",
       "      <td>0.888778</td>\n",
       "      <td>0.947375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>b396</td>\n",
       "      <td>b262</td>\n",
       "      <td>0.835989</td>\n",
       "      <td>0.895582</td>\n",
       "      <td>0.940091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>b396</td>\n",
       "      <td>b263</td>\n",
       "      <td>0.923633</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>0.972443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>b396</td>\n",
       "      <td>b264</td>\n",
       "      <td>0.894575</td>\n",
       "      <td>0.874000</td>\n",
       "      <td>0.961184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>b396</td>\n",
       "      <td>b277</td>\n",
       "      <td>0.890385</td>\n",
       "      <td>0.929719</td>\n",
       "      <td>0.968669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>b396</td>\n",
       "      <td>b278</td>\n",
       "      <td>0.884947</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.969765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tile A Tile B     Prec.    Recall       AUC\n",
       "0    b234   b220  0.909828  0.898000  0.970149\n",
       "1    b234   b277  0.906280  0.941767  0.976709\n",
       "2    b234   b278  0.909268  0.937626  0.978964\n",
       "3    b247   b220  0.918511  0.913000  0.974521\n",
       "4    b247   b234  0.717520  0.729000  0.822018\n",
       "5    b247   b264  0.898785  0.888000  0.964052\n",
       "6    b247   b277  0.880383  0.923695  0.968849\n",
       "7    b247   b278  0.892822  0.913481  0.971790\n",
       "8    b248   b220  0.898288  0.892000  0.963729\n",
       "9    b248   b234  0.722763  0.743000  0.813862\n",
       "10   b248   b247  0.713706  0.703000  0.818182\n",
       "11   b248   b262  0.845930  0.876506  0.944090\n",
       "12   b248   b263  0.921080  0.887000  0.970596\n",
       "13   b248   b264  0.886965  0.871000  0.958738\n",
       "14   b248   b277  0.890185  0.919679  0.970049\n",
       "15   b248   b278  0.901381  0.919517  0.974071\n",
       "16   b261   b220  0.884615  0.874000  0.947592\n",
       "17   b261   b234  0.910432  0.864000  0.963223\n",
       "18   b261   b247  0.880550  0.833000  0.945345\n",
       "19   b261   b248  0.879958  0.844689  0.946786\n",
       "20   b261   b262  0.624135  0.633534  0.661776\n",
       "21   b261   b263  0.914521  0.888000  0.969078\n",
       "22   b261   b264  0.899384  0.876000  0.959428\n",
       "23   b261   b277  0.857550  0.906627  0.950203\n",
       "24   b261   b278  0.861702  0.896378  0.947162\n",
       "25   b262   b220  0.870647  0.875000  0.945894\n",
       "26   b262   b234  0.909186  0.871000  0.962012\n",
       "27   b262   b247  0.865765  0.832000  0.941996\n",
       "28   b262   b263  0.924274  0.891000  0.971047\n",
       "29   b262   b264  0.910417  0.874000  0.959193\n",
       "30   b262   b277  0.853497  0.906627  0.946610\n",
       "31   b262   b278  0.845574  0.903421  0.945155\n",
       "32   b263   b220  0.914683  0.922000  0.979728\n",
       "33   b263   b234  0.894380  0.923000  0.970767\n",
       "34   b263   b247  0.898522  0.912000  0.970296\n",
       "35   b263   b264  0.568089  0.559000  0.591133\n",
       "36   b263   b277  0.924510  0.946787  0.982383\n",
       "37   b263   b278  0.924774  0.927565  0.981689\n",
       "38   b264   b220  0.910643  0.907000  0.974507\n",
       "39   b264   b234  0.876098  0.898000  0.964274\n",
       "40   b264   b277  0.914314  0.942771  0.974634\n",
       "41   b264   b278  0.915254  0.923541  0.974427\n",
       "42   b277   b220  0.947531  0.921000  0.983049\n",
       "43   b277   b278  0.741935  0.740443  0.849118\n",
       "44   b278   b220  0.934010  0.920000  0.983777\n",
       "45   b396   b220  0.909369  0.893000  0.971480\n",
       "46   b396   b234  0.843077  0.822000  0.922075\n",
       "47   b396   b247  0.852814  0.788000  0.915497\n",
       "48   b396   b248  0.836134  0.797595  0.905387\n",
       "49   b396   b261  0.844762  0.888778  0.947375\n",
       "50   b396   b262  0.835989  0.895582  0.940091\n",
       "51   b396   b263  0.923633  0.895000  0.972443\n",
       "52   b396   b264  0.894575  0.874000  0.961184\n",
       "53   b396   b277  0.890385  0.929719  0.968669\n",
       "54   b396   b278  0.884947  0.928571  0.969765"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(rows)[[\"Tile A\", \"Tile B\", \"Prec.\", \"Recall\", \"AUC\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prec.</th>\n",
       "      <th>Recall</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.868842</td>\n",
       "      <td>0.870562</td>\n",
       "      <td>0.939211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.074306</td>\n",
       "      <td>0.077063</td>\n",
       "      <td>0.073105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.568089</td>\n",
       "      <td>0.559000</td>\n",
       "      <td>0.591133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.855524</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.945619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.892822</td>\n",
       "      <td>0.893000</td>\n",
       "      <td>0.963729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.910424</td>\n",
       "      <td>0.919598</td>\n",
       "      <td>0.971635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.947531</td>\n",
       "      <td>0.946787</td>\n",
       "      <td>0.983777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Prec.     Recall        AUC\n",
       "count  55.000000  55.000000  55.000000\n",
       "mean    0.868842   0.870562   0.939211\n",
       "std     0.074306   0.077063   0.073105\n",
       "min     0.568089   0.559000   0.591133\n",
       "25%     0.855524   0.871000   0.945619\n",
       "50%     0.892822   0.893000   0.963729\n",
       "75%     0.910424   0.919598   0.971635\n",
       "max     0.947531   0.946787   0.983777"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
